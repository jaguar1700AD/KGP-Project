{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dir = 'C:\\\\Users\\\\shash\\\\Desktop\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_words = set()\n",
    "with open('D:\\\\KGP\\\\zip_words.txt','r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for word in csv_reader:\n",
    "        zip_words.add(word[0])\n",
    "zip_len = len(zip_words)\n",
    "zip_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set(set_string):\n",
    "    if set_string == 'set()':\n",
    "        return set()\n",
    "    else:\n",
    "        return ast.literal_eval(set_string)\n",
    "    \n",
    "def is_title(name):\n",
    "    if len(name) >= 3 and name[-3] == '(' and name[-2].isdigit() and name[-1] == ')':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def valid_entity(name):\n",
    "    if is_title(name) and (len(name) == 3 or name[-4] in string.punctuation):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_index = {}\n",
    "\n",
    "length = 0\n",
    "with open(global_dir + 'Extractions\\\\ent_index.txt','r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "        for entity in csv_reader:\n",
    "            ent_index[entity['name']] = int(entity['index'])\n",
    "            length = max([length, int(entity['index'])])\n",
    "length += 1 \n",
    "            \n",
    "ent_name = [0] * length\n",
    "with open(global_dir + 'Extractions\\\\ent_index.txt','r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "        for entity in csv_reader:\n",
    "            ent_name[int(entity['index'])] = entity['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [0] * length\n",
    "with open(global_dir + 'Extractions\\\\index_merged_extractions.txt','r') as csv_file:\n",
    "\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "\n",
    "        for entity in csv_reader:\n",
    "            _index = int(entity['index'])\n",
    "            entities[_index] = {}\n",
    "            entities[_index]['type'] = get_set(entity['type'])\n",
    "            entities[_index]['context_dict'] = ast.literal_eval(entity['context_dict'])\n",
    "            entities[_index]['co_occ'] = get_set(entity['co_occ'])\n",
    "            #entities[entity['name']]['zipf'] = get_set(entity['zipf_vector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a Manpage title entity -> entity\n",
    "# One way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 9 Progress: 20 / 20 \r"
     ]
    }
   ],
   "source": [
    "file_name = '1a.txt'\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for section in range(1, 10):\n",
    "\n",
    "        directory = global_dir + 'Extractions\\\\man' + str(section) + '\\\\'\n",
    "        anc_directory = global_dir + 'Extractions\\\\Anchors\\\\man' + str(section) + '\\\\'\n",
    "        all_files = os.listdir(directory)\n",
    "        n = 1\n",
    "\n",
    "        for file in all_files:\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Section : %d Progress: %d / %d \\r\" % (section, n, len(all_files)))\n",
    "            \n",
    "            title_set = set()\n",
    "            with open(anc_directory + file,'r') as csv_file:\n",
    "                csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "                for line in csv_reader:\n",
    "                    title_set.add(line['title'])\n",
    "\n",
    "            with open(directory + file,'r') as csv_file:\n",
    "                csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "                for entity in csv_reader:\n",
    "                    name = entity['name']\n",
    "                    if name not in title_set:\n",
    "                        for title in title_set:\n",
    "                            new_dict = {'ent1' : ent_index[title], 'ent2' : ent_index[name]}\n",
    "                            csv_writer.writerow(new_dict)\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowgrindd(1)\n",
      "isci(4)\n",
      "FFCLOCK(4)\n",
      "fmcs(1)\n",
      "af_svnum(3)\n",
      "KFAIL_POINT_RETURN_VOID(9)\n",
      "flow-merge(1)\n",
      "i386_get_ioperm(2)\n",
      "pfil(9)\n",
      "af_setattr(3)\n",
      "ALTER_TABLESPACE(7)\n",
      "afWriteMisc(3)\n",
      "AGPWRT(3)\n",
      "KFAIL_POINT_GOTO(9)\n",
      "acoshl(3)\n",
      "bus_generic_attach(9)\n",
      "af_isstdval(3)\n",
      "aoe-flush(8)\n",
      "apache2ctl(8)\n",
      "asc_demount(6)\n",
      "fanotify_init(2)\n",
      "altera_sdcard(4)\n",
      "abs(3)\n",
      "esp(4)\n",
      "KFAIL_POINT_CODE_FLAGS(9)\n",
      "cdist-type__sysctl(7)\n",
      "DECLARE_CC_MODULE(9)\n",
      "american-english-large(5)\n",
      "pfil_head_register(9)\n",
      "flogtool(1)\n",
      "io_queue_release(2)\n",
      "addrdsfig(3)\n",
      "kld(4)\n",
      ".cloginrc(5)\n",
      "flatpak-uninstall(1)\n",
      "bpf(9)\n",
      "amflush(8)\n",
      "blockattack(6)\n",
      "adjust_sample(3)\n",
      "fixscribeps(1)\n",
      "cyrus.conf(5)\n",
      "af_close(3)\n",
      "CREATE_COLLATION(7)\n",
      "band(2)\n",
      "bootchart.conf(5)\n",
      "edgar(6)\n",
      "af_retattr(3)\n",
      "audisp-remote.conf(5)\n",
      "cdk_marquee(3)\n",
      "combo.cfg(5)\n",
      "csm(3)\n",
      "4s-boss(8)\n",
      "resource_string_value(9)\n",
      "4store.conf(5)\n",
      "af_setbusy(3)\n",
      "FIGlet(6)\n",
      "cultivation(6)\n",
      "KFAIL_POINT_CODE_COND(9)\n",
      "af_establish(3)\n",
      "container-reconciler.conf(5)\n",
      "get_cyclecount(9)\n",
      "afs(5)\n",
      "af_retnumattr(3)\n",
      "aa_fastrender(3)\n",
      "connect(2)\n",
      "appschema(5)\n",
      "canadian-english-small(5)\n",
      "cdist-type__file(7)\n",
      "af_retuserattr(3)\n",
      "af_sstate(3)\n",
      "dgit-sponsorship(7)\n",
      "fits-column-merge(1)\n",
      "ixl(4)\n",
      "getpeername(2)\n",
      "acosh(3)\n",
      "0ad(6)\n",
      "DECLARE_GEOM_CLASS(9)\n",
      "amavisd-milter(8)\n",
      "dracut.modules(7)\n",
      "cage(6)\n",
      "arp2ethers(8)\n",
      "deb-control(5)\n",
      "bcfg2.conf(5)\n",
      "hv_vss(4)\n",
      "pfil_add_hook(9)\n",
      "augenrules(8)\n",
      "cdist-type__consul(7)\n",
      "auvirt(8)\n",
      "pfil_rlock(9)\n",
      "msgsnd(2)\n",
      "intel-virtual-output(4)\n",
      "cmake-server(7)\n",
      "acl_set_fd(3)\n",
      "ALTER_FUNCTION(7)\n",
      "mkdirat(2)\n",
      "add-apt-key(8)\n",
      "cgoban(6)\n",
      "getsid(2)\n",
      "resource_int_value(9)\n",
      "vdrop(9)\n",
      "style(9)\n",
      "daemon(7)\n",
      "mkdir(2)\n",
      "move_pages(2)\n",
      "dhcpd.conf(5)\n",
      "decayscreen(6)\n",
      "kldload(2)\n",
      "freeciv-server(6)\n",
      "cdist-type__install_stage(7)\n",
      "cdist-type__postfix_postconf(7)\n",
      "resource_long_value(9)\n",
      "getgid(2)\n",
      "aa_mmheight(3)\n",
      "af_freeattrbuf(3)\n",
      "ccd(4)\n",
      "ath_hal(4)\n",
      "addlocap(3)\n",
      "ds3231(4)\n",
      "addbddcircuitin(3)\n",
      "csv2_txt(5)\n",
      "cloudlife(6)\n",
      "pmap_page_init(9)\n",
      "accept_filt_get(9)\n",
      "adw(4)\n",
      "af_rm(3)\n",
      "aac(4)\n",
      "CCV(9)\n",
      "distort(6)\n",
      "afSeekMisc(3)\n",
      "pfil_remove_hook(9)\n",
      "device_set_driver(9)\n",
      "DEBUG_FP(9)\n",
      "apt(8)\n",
      "ace-of_penguins(6)\n",
      "fchdir(2)\n",
      "charmap(5)\n",
      "device_delete_child(9)\n",
      "/etc/decnet.conf(5)\n",
      "module(9)\n",
      "tap(4)\n",
      "brutalchess(6)\n",
      "crawl(6)\n",
      "aa_change_hat(2)\n",
      "af_open(3)\n",
      "bootchart.conf.d(5)\n",
      "VOP_GETACL(9)\n",
      "_llseek(2)\n",
      "cxgb(4)\n",
      "elvi(1)\n",
      "amber_ascii_export(7)\n",
      "buffindexed.conf(5)\n",
      "pfil_head_unregister(9)\n",
      "amserverconfig(8)\n",
      "i386_set_ioperm(2)\n",
      "anagramarama(6)\n",
      "ioctl(2)\n",
      "fiu-ctrl(1)\n",
      "field_functor(2)\n",
      "accept_filt_del(9)\n",
      "KFAIL_POINT_ERROR(9)\n",
      "acl_delete_entry(3)\n",
      "af_allattrs(3)\n",
      "avgrc(5)\n",
      "ABORT(7)\n",
      "fntsample(1)\n",
      "epoll_create1(2)\n",
      "getegid(2)\n",
      "aprsmon(8)\n",
      "fill-aa(1)\n",
      "pfil_run_hooks(9)\n",
      "aa_resizehandler(3)\n",
      "ion(1)\n",
      "flashproxy-reg-email(1)\n",
      "vm_page_grab(9)\n",
      "afReadMisc(3)\n",
      "vfs_unbusy(9)\n",
      "chdir(2)\n",
      "glxsb(4)\n",
      "ciss(4)\n",
      "vhold(9)\n",
      "astdb2bdb(8)\n",
      "pfil_wunlock(9)\n",
      "msgrcv(2)\n",
      "attributes(7)\n",
      "capabilities(7)\n",
      "af_crkey(3)\n",
      "flatpak-config(1)\n",
      "flickrdf(1)\n",
      "aio_fsync(2)\n",
      "folders(1)\n",
      "af_rettimeattr(3)\n",
      "acoshf(3)\n",
      "eToys(6)\n",
      "CREATE_SCHEMA(7)\n",
      "atunnel(6)\n",
      "fail_point(9)\n",
      "statfs(2)\n",
      "discgrp(5)\n",
      "KFAIL_POINT_CODE(9)\n",
      "KFAIL_POINT_SLEEP_CALLBACKS(9)\n",
      "filter_seer(1)\n",
      "pfil_head_get(9)\n",
      "af_saverev(3)\n",
      "ieee80211_regdomain(9)\n",
      "axMail(8)\n",
      "KFAIL_POINT_RETURN(9)\n",
      "firewall-offline-cmd(1)\n",
      "airbase-ng(8)\n",
      "mod_cc(9)\n",
      "cdk_scale(3)\n",
      "folder(1)\n",
      "flow(6)\n",
      "af_freeattr(3)\n",
      "accept_filt_generic_mod_event(9)\n",
      "crypto(7)\n",
      "cdist-type__package_emerge(7)\n",
      "aa-mergeprof(8)\n",
      "af_newgen(3)\n",
      "vm_map_init(9)\n",
      "otus(4)\n",
      "accept_filt_add(9)\n",
      "vdropl(9)\n",
      "bhndb(4)\n",
      "pfil_runlock(9)\n",
      "accept_filter(9)\n",
      "af_restore(3)\n",
      "pfil_wlock(9)\n",
      "findhost.cgi(1)\n",
      "berusky(6)\n",
      "epoll_create(2)\n",
      "fl-run-test(1)\n",
      "fstatfs(2)\n",
      "vxge(4)\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "file_name = '1a.txt'\n",
    "title_set = set()\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name,'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "    for line in csv_reader:\n",
    "        title_set.add(line['ent1'])\n",
    "        \n",
    "for title in title_set:\n",
    "    print(ent_name[int(title)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b +- k window\n",
    "# One way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '1b.txt'\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "    for index1 in range(length):\n",
    "        for index2 in entities[index1]['co_occ']:\n",
    "            new_dict = {'ent1' : index1, 'ent2' : index2}\n",
    "            csv_writer.writerow(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Anchor link\n",
    "# One way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 9 Progress: 20 / 20 \r"
     ]
    }
   ],
   "source": [
    "file_name = '2.txt'\n",
    "with open(global_dir + '\\\\Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for section in range(1, 10):\n",
    "\n",
    "        directory = global_dir + 'Extractions\\\\Anchors\\\\man' + str(section) + '\\\\'\n",
    "        all_files = os.listdir(directory)\n",
    "        n = 1\n",
    "\n",
    "        for file in all_files:\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Section : %d Progress: %d / %d \\r\" % (section, n, len(all_files)))\n",
    "\n",
    "            with open(directory + file,'r') as csv_file:\n",
    "                csv_reader = csv.DictReader(csv_file, delimiter = '\\t') \n",
    "                for line in csv_reader:\n",
    "                    for link in get_set(line['links']):\n",
    "                        new_dict = {'ent1' : ent_index[line['title']], 'ent2' : ent_index[link]}\n",
    "                        csv_writer.writerow(new_dict)\n",
    "\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Similar context_dict link from holling\n",
    "# Two way relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_list = [0] * length\n",
    "\n",
    "for i in range(length):\n",
    "    cont_list[i] = entities[i]['context_dict']\n",
    "        \n",
    "num = len(cont_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(30)\n",
    "random.shuffle(cont_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate occurence_count for all entities\n",
    "occ_count = {}\n",
    "for index in range(length):\n",
    "    entity = entities[index]\n",
    "    occ_count[index] = 0\n",
    "    for context in entity['context_dict']:\n",
    "        occ_count[index] += entity['context_dict'][context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4240 / 4240 \r"
     ]
    }
   ],
   "source": [
    "file_name = '3.txt'\n",
    "with open(global_dir + 'Extractions\\\\Graph\\\\' + file_name, 'a', newline = '') as file_int:\n",
    "    \n",
    "    fieldnames = ['ent1','ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "    \n",
    "    threshold = 0.9 # 2 entities will be connected by an edge if no of common contexts > threshold * no of contexts for both entities\n",
    "    lim1 = 0\n",
    "    lim2 = num\n",
    "    for i in range(lim1, lim2):\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"%d / %d \\r\" % (i - lim1 + 1, lim2 - lim1))\n",
    "\n",
    "        cont1 = cont_list[i]\n",
    "\n",
    "        for j in range(i + 1, num):\n",
    "\n",
    "            cont2 = cont_list[j]\n",
    "\n",
    "            n1 = 0 # No of common contexts of entity1\n",
    "            n2 = 0 # No of common contexts of entity2\n",
    "\n",
    "            for context in cont1:\n",
    "                if context in cont2:\n",
    "                    n1 += cont1[context]\n",
    "                    n2 += cont2[context]\n",
    "\n",
    "\n",
    "            if (n1 >= threshold * occ_count[i]) and (n2 >= threshold * occ_count[j]):\n",
    "                new_dict = {'ent1' : i, 'ent2' : j}\n",
    "                csv_writer.writerow(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 zipf based relation\n",
    "# Two way relation\n",
    "\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_ent_set = set()\n",
    "four_index = {}\n",
    "\n",
    "max_ent_len = 0\n",
    "for name in ent_name:\n",
    "    cand = name.replace(' ', '')\n",
    "    four_ent_set.add(cand)\n",
    "    four_index[cand] = ent_index[name]\n",
    "    max_ent_len = max([max_ent_len, len(nltk.word_tokenize(name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entity pairs between +- window_size\n",
    "# word_list is list of [entity_index, start_word_index, entity_length]\n",
    "# Remove pairs between the same words in the end\n",
    "\n",
    "def find_pairs(word_list):\n",
    "    n = len(word_list)\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        start_limit_up = word_list[i][1] + word_list[i][2] + window_size\n",
    "        start_limit_down = word_list[i][1] + word_list[i][2]\n",
    "        j = i + 1\n",
    "        while j < n and word_list[j][1] <= start_limit_up and word_list[j][1] >= start_limit_down:\n",
    "            pairs.append([i, j])\n",
    "            j += 1\n",
    "    \n",
    "    for ind, pair in enumerate(pairs):\n",
    "        if word_list[pair[0]][0] == word_list[pair[1]][0]:\n",
    "            pairs[ind] = None\n",
    "            \n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 9 Progress: 20 / 20 \r"
     ]
    }
   ],
   "source": [
    "file_name = '4.txt'\n",
    "with open(global_dir + '\\\\Extractions\\\\Graph\\\\' + file_name, 'w') as file_int:\n",
    "    fieldnames = ['ent1','relation', 'ent2']\n",
    "    csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for section in range(1, 10):\n",
    "        directory = 'D:\\\\KGP\\\\man' + str(section) + '\\\\'\n",
    "        all_files = os.listdir(directory)\n",
    "        all_files = {all_files[i] for i in range(0,400,20)}\n",
    "        \n",
    "        n = 1\n",
    "        for file in all_files:\n",
    "            \n",
    "            with open(directory + file, 'rb') as sauce:\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stdout.write(\"Section : %d Progress: %d / %d \\r\" % (section, n, len(all_files)))\n",
    "\n",
    "                soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "\n",
    "                sentences = nltk.sent_tokenize(soup.get_text())\n",
    "\n",
    "                for sentence in sentences:\n",
    "\n",
    "                    word_list = []\n",
    "                    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "                    for start in range(len(words)):\n",
    "                        for length in range(1, min(len(words) - start, max_ent_len) + 1):\n",
    "\n",
    "                            candidate = ''.join(words[start: start + length])\n",
    "\n",
    "                            if candidate in four_ent_set:\n",
    "                                word_list.append([four_index[candidate], start, length])\n",
    "\n",
    "                            if is_title(candidate):\n",
    "                                cand = candidate[:-3]\n",
    "                                if cand in four_ent_set:\n",
    "                                    word_list.append([four_index[cand], start, length])\n",
    "                            else:\n",
    "                                cand = candidate + '(' + str(section) + ')'\n",
    "                                if cand in four_ent_set:\n",
    "                                    word_list.append([four_index[cand], start, length])\n",
    "                    \n",
    "                    pairs = find_pairs(word_list)\n",
    "                    \n",
    "                    for pair in pairs:\n",
    "                        \n",
    "                        if pair == None:\n",
    "                            continue\n",
    "                        \n",
    "                        list0 = word_list[pair[0]]\n",
    "                        list1 = word_list[pair[1]]\n",
    "                        \n",
    "                        start_ind = list0[1] + list0[2]\n",
    "                        end_in = list1[1]\n",
    "                        relation = set(words[start_ind : end_in]) & zip_words\n",
    "                        \n",
    "                        try:\n",
    "                            #new_dict = {'file' : words, 'ent1' : ent_name[list0[0]], 'relation' : relation, 'ent2' : ent_name[list1[0]]}\n",
    "                            new_dict = {'ent1' : list0[0], 'relation' : relation, 'ent2' : list1[0]}\n",
    "                            csv_writer.writerow(new_dict)\n",
    "                        except:\n",
    "                            pass\n",
    "                n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
