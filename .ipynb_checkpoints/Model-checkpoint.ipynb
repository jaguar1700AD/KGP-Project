{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCharVocab(fname):\n",
    "    charVocab={}\n",
    "    with open(fname, 'rt') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            char_id = int(fields[0])\n",
    "            ch = fields[1]\n",
    "            charVocab[ch] = char_id\n",
    "    return charVocab\n",
    "\n",
    "def loadVocab(fname):\n",
    "    vocab={}\n",
    "    idf={}\n",
    "    with open(fname, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode('utf-8').strip()\n",
    "            #line = line.strip()\n",
    "            fields = line.split('\\t')\n",
    "            term_id = int(fields[0])\n",
    "            vocab[fields[1]] = term_id\n",
    "            total_doc = int(fields[4])\n",
    "            doc_freq = int(fields[3])\n",
    "            idf[term_id] = math.log((0.5+total_doc)/(0.5+doc_freq))\n",
    "    return vocab, idf\n",
    "\n",
    "def toVec(tokens, vocab, maxlen):\n",
    "    n = len(tokens)\n",
    "    length = 0\n",
    "    vec=[]\n",
    "    for i in range(n):\n",
    "        length += 1\n",
    "        if tokens[i] in vocab:\n",
    "            vec.append(vocab[tokens[i]])\n",
    "        else:\n",
    "            vec.append(vocab[\"UNKNOWN\"])\n",
    "\n",
    "    return length, np.array(vec)\n",
    "\n",
    "def loadAnswers(fname, vocab, maxlen):\n",
    "    answers={}\n",
    "    with open(fname, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode('utf-8').strip()\n",
    "            fields = line.split('\\t')\n",
    "            if len(fields) != 2:\n",
    "                print(\"WRONG LINE: {}\".format(line))\n",
    "                a_text = 'UNKNOWN'\n",
    "            else:\n",
    "                a_text = fields[1]\n",
    "            tokens = a_text.split(' ')\n",
    "            len1, vec = toVec(tokens[:maxlen], vocab, maxlen)\n",
    "            #answers[fields[0]] = (len1, vec, tokens[:maxlen])\n",
    "            answers[fields[0]] = vec\n",
    "    return answers\n",
    "\n",
    "def loadDataset(fname, vocab, maxlen, answers):\n",
    "    dataset=[]\n",
    "    with open(fname, 'rb') as f:\n",
    "        for line in f:\n",
    "            line = line.decode('utf-8').strip()\n",
    "            fields = line.split('\\t')\n",
    "            q_id = fields[0]\n",
    "            question_text = fields[1]\n",
    "            q_tokens = question_text.split(' ')\n",
    "            q_len, q_vec = toVec(q_tokens[:maxlen], vocab, maxlen)\n",
    "            if fields[3] != \"NA\":\n",
    "                neg_ids = [id for id in fields[3].split('|')]\n",
    "                for aid in neg_ids:\n",
    "                    #a_len, a_vec, a_tokens = answers[aid]\n",
    "                    #dataset.append((q_id, q_len, q_vec, aid, a_len, a_vec, 0.0, q_tokens[:maxlen], a_tokens))\n",
    "                    a_vec = answers[aid]\n",
    "                    dataset.append((q_vec, a_vec, 0.0))\n",
    "\n",
    "            if fields[2] != \"NA\":\n",
    "                pos_ids = [id for id in fields[2].split('|')]\n",
    "                for aid in pos_ids:\n",
    "                    #a_len, a_vec, a_tokens = answers[aid]\n",
    "                    #dataset.append((q_id, q_len, q_vec, aid, a_len, a_vec, 1.0, q_tokens[:maxlen], a_tokens))\n",
    "                    a_vec = answers[aid]\n",
    "                    dataset.append((q_vec, a_vec, 1.0))\n",
    "    return dataset\n",
    "\n",
    "def charVec(tokens, charVocab, maxlen, maxWordLength):\n",
    "    n = len(tokens)\n",
    "    if n > maxlen:\n",
    "        n = maxlen\n",
    "\n",
    "    chars =  np.zeros((maxlen, maxWordLength), dtype=np.int32)\n",
    "    word_lengths = np.ones(maxlen, dtype=np.int32)\n",
    "    for i in range(n):\n",
    "        token = tokens[i][:maxWordLength]\n",
    "        word_lengths[i] = len(token)\n",
    "        row = chars[i]\n",
    "        for idx, ch in enumerate(token):\n",
    "            if ch in charVocab:\n",
    "                row[idx] = charVocab[ch]\n",
    "\n",
    "    return chars, word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vocab):\n",
    "    print(\"get_embedding\")\n",
    "    initializer = load_word_embeddings(vocab, FLAGS.embedding_dim)\n",
    "    return tf.constant(initializer, name=\"word_embedding\")\n",
    "\n",
    "def get_char_embedding(charVocab):\n",
    "    char_size = len(charVocab)\n",
    "    embeddings = np.zeros((char_size, char_size), dtype='float32')\n",
    "    for i in range(1, char_size):\n",
    "        embeddings[i, i] = 1.0\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def load_embed_vectors(fname, dim):\n",
    "    vectors = {}\n",
    "    for line in open(fname, 'rb'):\n",
    "        items = line.decode('utf-8').strip().split(' ')\n",
    "        if len(items[0]) <= 0:\n",
    "            continue\n",
    "        vec = [float(items[i]) for i in range(1, dim+1)]\n",
    "        vectors[items[0]] = vec\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def load_word_embeddings(vocab, dim):\n",
    "    vectors = load_embed_vectors(path_word_embed, dim)\n",
    "    vocab_size = len(vocab)\n",
    "    embeddings = np.zeros((vocab_size, dim), dtype='float32')\n",
    "    for word, code in vocab.items():\n",
    "        if word in vectors:\n",
    "            embeddings[code] = vectors[word]\n",
    "        #else:\n",
    "        #    embeddings[code] = np.random.uniform(-0.25, 0.25, dim) \n",
    "\n",
    "    return embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_char_vocab = 'D:/Ubuntu_corpus_V2/char_vocab.txt'\n",
    "path_vocab = 'D:/Ubuntu_corpus_V2/vocab.txt'\n",
    "path_ans = 'D:/Ubuntu_corpus_V2/answers.txt'\n",
    "path_train = 'D:/Ubuntu_corpus_V2/train.txt'\n",
    "path_valid = 'D:/Ubuntu_corpus_V2/valid.txt'\n",
    "path_word_embed = 'D:/Ubuntu_corpus_V2/glove_42B_300d_vec_plus_word2vec_100.txt'\n",
    "\n",
    "SEQ_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "charVocab = loadCharVocab(path_char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, idf = loadVocab(path_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_embeddings = load_word_embeddings(vocab, 400)\n",
    "word_embeddings = np.zeros((725503, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embeddings = get_char_embedding(charVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = loadAnswers(path_ans, vocab, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = loadDataset(path_train, vocab, SEQ_LEN, answers)\n",
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = loadDataset(path_valid, vocab, SEQ_LEN, answers)\n",
    "len(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_embeddings[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "def lstm_layer(inputs, input_seq_len, rnn_size, dropout_keep_prob, scope, scope_reuse=False):\n",
    "    with tf.variable_scope(scope, reuse=scope_reuse) as vs:\n",
    "        fw_cell = tf.contrib.rnn.LSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True, reuse=scope_reuse)\n",
    "        fw_cell  = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=dropout_keep_prob)\n",
    "        bw_cell = tf.contrib.rnn.LSTMCell(rnn_size, forget_bias=1.0, state_is_tuple=True, reuse=scope_reuse)\n",
    "        bw_cell  = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=dropout_keep_prob)\n",
    "        rnn_outputs, rnn_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, cell_bw=bw_cell,\n",
    "                                                                inputs=inputs,\n",
    "                                                                sequence_length=input_seq_len,\n",
    "                                                                dtype=tf.float32)\n",
    "        return rnn_outputs, rnn_states\n",
    "\n",
    "\n",
    "def question_answer_similarity_matrix(question, answer):\n",
    "    q_len = question.get_shape()[1].value\n",
    "    a_len = answer.get_shape()[1].value\n",
    "    dim = question.get_shape()[2].value\n",
    "\n",
    "    q_w = question\n",
    "\n",
    "    #answer : batch_size * a_len * dim\n",
    "    #[batch_size, dim, q_len]\n",
    "    q2 = tf.transpose(q_w, perm=[0,2,1])\n",
    "\n",
    "    #[batch_size, a_len, q_len]\n",
    "    similarity = tf.matmul(answer, q2, name='similarity_matrix')\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def self_attended(similarity_matrix, inputs):\n",
    "    #similarity_matrix: [batch_size, len, len]\n",
    "    #inputs: [batch_size, len, dim]\n",
    "\n",
    "    attended_w = tf.nn.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "    #[batch_size, len, dim]\n",
    "    attended_out = tf.matmul(attended_w, inputs)\n",
    "    return attended_out\n",
    "\n",
    "def attended_answers(similarity_matrix, questions):\n",
    "    #similarity_matrix: [batch_size, a_len, q_len]\n",
    "    #questions: [batch_size, q_len, dim]\n",
    "\n",
    "    #[batch_size, a_len, q_len]\n",
    "    attention_weight_for_q = tf.nn.softmax(similarity_matrix, dim=-1)\n",
    "\n",
    "    #[batch_size, a_len, dim]\n",
    "    attended_answers = tf.matmul(attention_weight_for_q, questions)\n",
    "    return attended_answers\n",
    "\n",
    "def attended_questions(similarity_matrix, answers):\n",
    "    #similarity_matrix: [batch_size, a_len, q_len]\n",
    "    #answers: [batch_size, a_len, dim]\n",
    "\n",
    "    #[batch_size, q_len, a_len]\n",
    "    attention_weight_for_a = tf.nn.softmax(tf.transpose(similarity_matrix, perm=[0,2,1]), dim=-1)\n",
    "\n",
    "    #[batch_size, q_len, dim]\n",
    "    attended_questions = tf.matmul(attention_weight_for_a, answers)\n",
    "    return attended_questions\n",
    "\n",
    "\n",
    "class ESIM(object):\n",
    "    def __init__(\n",
    "      self, sequence_length, vocab_size, embedding_size, vocab, rnn_size, maxWordLength, charVocab, l2_reg_lambda=0.0):\n",
    "\n",
    "        #question\n",
    "        self.question = tf.placeholder(tf.int32, [None, sequence_length], name=\"question\")\n",
    "        #answer\n",
    "        self.answer = tf.placeholder(tf.int32, [None, sequence_length], name=\"answer\")\n",
    "\n",
    "        self.target = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "\n",
    "        self.target_loss_weight = tf.placeholder(tf.float32, [None], name=\"target_weight\")\n",
    "\n",
    "        self.question_len = tf.placeholder(tf.int32, [None], name=\"question_len\")\n",
    "        self.answer_len = tf.placeholder(tf.int32, [None], name=\"answer_len\")\n",
    "\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.extra_feature = tf.placeholder(tf.float32, [None, 2], name=\"extra_feature\")\n",
    "\n",
    "        self.q_word_feature = tf.placeholder(tf.float32, [None, sequence_length, 2], name=\"question_word_feature\")\n",
    "        self.a_word_feature = tf.placeholder(tf.float32, [None, sequence_length, 2], name=\"answer_word_feature\")\n",
    "\n",
    "        self.q_charVec = tf.placeholder(tf.int32, [None, sequence_length, maxWordLength], name=\"question_char\")\n",
    "        self.q_charLen = tf.placeholder(tf.int32, [None, sequence_length], name=\"question_char_len\")\n",
    "\n",
    "        self.a_charVec = tf.placeholder(tf.int32, [None, sequence_length, maxWordLength], name=\"answer_char\")\n",
    "        self.a_charLen =  tf.placeholder(tf.int32, [None, sequence_length], name=\"answer_char_len\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = get_embeddings(vocab) \n",
    "            question_embedded = tf.nn.embedding_lookup(W, self.question)\n",
    "            answer_embedded = tf.nn.embedding_lookup(W, self.answer)\n",
    "\n",
    "        with tf.device('/cpu:0'), tf.name_scope('char_embedding'):\n",
    "            char_W = get_char_embedding(charVocab)\n",
    "            #[batch_size, q_len, maxWordLength, char_dim]\n",
    "            question_char_embedded = tf.nn.embedding_lookup(char_W, self.q_charVec)\n",
    "\n",
    "            #[batch_size, a_len, maxWordLength, char_dim]\n",
    "            answer_char_embedded   = tf.nn.embedding_lookup(char_W, self.a_charVec)\n",
    "\n",
    "\n",
    "        charRNN_size=40\n",
    "        charRNN_name=\"char_RNN\"\n",
    "        char_dim = question_char_embedded.get_shape()[3].value\n",
    "        question_char_embedded = tf.reshape(question_char_embedded, [-1, maxWordLength, char_dim])\n",
    "        question_char_len      = tf.reshape(self.q_charLen, [-1])\n",
    "        answer_char_embedded  = tf.reshape(answer_char_embedded, [-1, maxWordLength, char_dim])\n",
    "        answer_char_len       = tf.reshape(self.a_charLen, [-1])\n",
    "\n",
    "        char_rnn_output1, char_rnn_states1 = lstm_layer(question_char_embedded, question_char_len, charRNN_size, self.dropout_keep_prob, charRNN_name, scope_reuse=False)\n",
    "        char_rnn_output2, char_rnn_states2 = lstm_layer(answer_char_embedded, answer_char_len, charRNN_size, self.dropout_keep_prob, charRNN_name, scope_reuse=True)\n",
    "\n",
    "        question_char_state = tf.concat(axis=1, values=[char_rnn_states1[0].h, char_rnn_states1[1].h])\n",
    "        char_embed_dim = 2 * charRNN_size\n",
    "        question_char_state = tf.reshape(question_char_state, [-1, sequence_length, char_embed_dim])\n",
    "\n",
    "        answer_char_state = tf.concat(axis=1, values=[char_rnn_states2[0].h, char_rnn_states2[1].h] )\n",
    "        answer_char_state = tf.reshape(answer_char_state, [-1, sequence_length, char_embed_dim])\n",
    "\n",
    "        rnn_scope_name = \"bidirectional_rnn\"\n",
    "\n",
    "        question_embedded = tf.concat(axis=2, values=[question_embedded, question_char_state])\n",
    "        answer_embedded   = tf.concat(axis=2, values=[answer_embedded, answer_char_state])\n",
    "\n",
    "        print(\"shape of question_embedded\");\n",
    "        print(question_embedded.get_shape())\n",
    "\n",
    "        rnn_output1, rnn_states1 = lstm_layer(question_embedded, self.question_len, rnn_size, self.dropout_keep_prob, rnn_scope_name, scope_reuse=False)\n",
    "        rnn_output2, rnn_states2 = lstm_layer(answer_embedded, self.answer_len, rnn_size, self.dropout_keep_prob, rnn_scope_name, scope_reuse=True)\n",
    "\n",
    "        #[batch_size, question_len, dim]\n",
    "        question_output = tf.concat(axis=2, values=rnn_output1)\n",
    "\n",
    "        #[batch_size, answer_len, dim]\n",
    "        answer_output   = tf.concat(axis=2, values=rnn_output2)\n",
    "\n",
    "\n",
    "        HOPS = 1\n",
    "\n",
    "        for i in range(HOPS):\n",
    "            #[batch_size, answer_len, question_len]\n",
    "            similarity = question_answer_similarity_matrix(question_output, answer_output)\n",
    "\n",
    "            #[batch_size, answer_len, dim]\n",
    "            attended_answer_output = attended_answers(similarity, question_output)\n",
    "\n",
    "            #[batch_size, question_len, dim]\n",
    "            attended_question_output = attended_questions(similarity, answer_output)\n",
    "\n",
    "            m_a = tf.concat(axis=2, values=[answer_output, attended_answer_output, tf.multiply(answer_output, attended_answer_output), answer_output-attended_answer_output])\n",
    "            m_q = tf.concat(axis=2, values=[question_output, attended_question_output, tf.multiply(question_output, attended_question_output), question_output-attended_question_output])\n",
    "            rnn_scope_layer2 = 'bidirectional_rnn_{}'.format(i+2)\n",
    "            rnn_size_layer_2 = rnn_size\n",
    "            rnn_output_q_2, rnn_states_q_2 = lstm_layer(m_q, self.question_len, rnn_size_layer_2, self.dropout_keep_prob, rnn_scope_layer2, scope_reuse=False)\n",
    "            rnn_output_a_2, rnn_states_a_2 = lstm_layer(m_a, self.answer_len, rnn_size_layer_2, self.dropout_keep_prob, rnn_scope_layer2, scope_reuse=True)\n",
    "\n",
    "            question_output = tf.concat(axis=2, values=rnn_output_q_2)\n",
    "            answer_output   = tf.concat(axis=2, values=rnn_output_a_2)\n",
    "\n",
    "\n",
    "        question_output_2 = tf.concat(axis=2, values=rnn_output_q_2)\n",
    "        answer_output_2   = tf.concat(axis=2, values=rnn_output_a_2)\n",
    "\n",
    "        final_question_max = tf.reduce_max(question_output_2, axis=1)\n",
    "        final_answer_max   = tf.reduce_max(answer_output_2, axis=1)\n",
    "\n",
    "        layer_q_last_state = tf.concat(axis=1, values=[rnn_states_q_2[0].h, rnn_states_q_2[1].h])\n",
    "        layer_a_last_state = tf.concat(axis=1, values=[rnn_states_a_2[0].h, rnn_states_a_2[1].h])\n",
    "\n",
    "        with tf.device('/gpu:0'), tf.name_scope(\"convolution-1\"):\n",
    "\n",
    "            joined_feature =  tf.concat(axis=1, values=[final_question_max, final_answer_max, layer_q_last_state, layer_a_last_state])\n",
    "            print(\"shape of joined feature\")\n",
    "            print(joined_feature.get_shape())\n",
    "\n",
    "            hidden_input_size = joined_feature.get_shape()[1].value\n",
    "\n",
    "            hidden_output_size = 256\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(l2_reg_lambda)\n",
    "\n",
    "            #regularizer = None\n",
    "            with tf.variable_scope(\"projected_layer2\", regularizer=regularizer):\n",
    "                full_out = tf.contrib.layers.fully_connected(joined_feature, hidden_output_size,\n",
    "                                                                activation_fn=tf.nn.relu,\n",
    "                                                                reuse=False,\n",
    "                                                                trainable=True,\n",
    "                                                                scope=\"projected_layer\")\n",
    "\n",
    "            #full_out = tf.concat(axis=1, values=[full_out, self.extra_feature])\n",
    "            last_weight_dim = full_out.get_shape()[1].value\n",
    "            print(\"last_weight_dim: {}\".format(last_weight_dim))\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bias\")\n",
    "            s_w = tf.get_variable(\"s_w\", shape=[last_weight_dim, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            logits = tf.matmul(full_out, s_w) + bias\n",
    "\n",
    "            print(\"logits shape\")\n",
    "            print(logits.get_shape())\n",
    "\n",
    "            logits = tf.squeeze(logits, [1])\n",
    "\n",
    "            self.probs = tf.sigmoid(logits, name=\"prob\")\n",
    "\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.target)\n",
    "\n",
    "            losses = tf.multiply(losses, self.target_loss_weight)\n",
    "\n",
    "            self.mean_loss = tf.reduce_mean(losses, name=\"mean_loss\") + l2_reg_lambda * l2_loss + sum(\n",
    "                                                              tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.sign(self.probs - 0.5), tf.sign(self.target - 0.5))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ESIM(sequence_length = sequence_length, vocab_size = vocab_size, embedding_size, vocab, rnn_size, maxWordLength, charVocab, l2_reg_lambda=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "vocab_size = len(vocab)\n",
    "char_vocab_size = len(charVocab)\n",
    "char_dim = char_vocab_size\n",
    "embedding_size = 400\n",
    "#vocab\n",
    "rnn_size = 40\n",
    "maxWordLength = 10\n",
    "#charVocab\n",
    "l2_reg_lambda=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Reshape, Embedding, LSTM, Bidirectional, Concatenate, TimeDistributed\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = Input(shape=(sequence_length, ), dtype='int32')\n",
    "answer = Input(shape=(sequence_length, ), dtype='int32')\n",
    "\n",
    "target = Input(shape=(), dtype='float32')\n",
    "target_loss_weight = Input(shape=(), dtype='float32')\n",
    "\n",
    "question_len = Input(shape=(), dtype='int32')\n",
    "answer_len = Input(shape=(), dtype='int32')\n",
    "\n",
    "dropout_keep_prob = Input(shape=(), dtype='float32')\n",
    "\n",
    "q_charVec = Input(shape=(sequence_length, maxWordLength), dtype='int32')\n",
    "q_charLen = Input(shape=(sequence_length,), dtype='int32')\n",
    "\n",
    "a_charVec = Input(shape=(sequence_length, maxWordLength), dtype='int32')\n",
    "a_charLen = Input(shape=(sequence_length,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shash\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "question_embedded = Embedding(vocab_size, 400, input_length=sequence_length, weights=[word_embeddings], trainable = False)(question)\n",
    "answer_embedded = Embedding(vocab_size, 400, input_length=sequence_length, weights=[word_embeddings], trainable = False)(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_char_embedded = TimeDistributed(Embedding(char_vocab_size, char_dim, input_length=maxWordLength, weights=[char_embeddings], trainable = False))(q_charVec)\n",
    "answer_char_embedded = TimeDistributed(Embedding(char_vocab_size, char_dim, input_length=maxWordLength, weights=[char_embeddings], trainable = False))(a_charVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"list\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-dcc7158499d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# _, ah1, _, ah2, _ = TimeDistributed(Bidirectional(LSTM(charRNN_size, return_state = True)))(answer_char_embedded)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaga1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcharRNN_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_char_embedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mdaga2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcharRNN_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer_char_embedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    250\u001b[0m                 \u001b[0muses_learning_phase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;31m# Shape: (num_samples, timesteps, ...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m             output_shape = self._get_shape_tuple(\n\u001b[0;32m    254\u001b[0m                 (-1, input_length), y, 1, output_shape[2:])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mchild_output_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mtimesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchild_output_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mchild_output_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate tuple (not \"list\") to tuple"
     ]
    }
   ],
   "source": [
    "# charRNN_size=40\n",
    "# charRNN_name=\"char_RNN\"\n",
    "# char_dim = question_char_embedded.get_shape()[3].value\n",
    "# question_char_embedded = tf.reshape(question_char_embedded, [-1, maxWordLength, char_dim])\n",
    "# question_char_len      = tf.reshape(self.q_charLen, [-1])\n",
    "# answer_char_embedded  = tf.reshape(answer_char_embedded, [-1, maxWordLength, char_dim])\n",
    "# answer_char_len       = tf.reshape(self.a_charLen, [-1])\n",
    "\n",
    "# char_rnn_output1, char_rnn_states1 = lstm_layer(question_char_embedded, question_char_len, charRNN_size, self.dropout_keep_prob, charRNN_name, scope_reuse=False)\n",
    "# char_rnn_output2, char_rnn_states2 = lstm_layer(answer_char_embedded, answer_char_len, charRNN_size, self.dropout_keep_prob, charRNN_name, scope_reuse=True)\n",
    "\n",
    "# question_char_state = tf.concat(axis=1, values=[char_rnn_states1[0].h, char_rnn_states1[1].h])\n",
    "# char_embed_dim = 2 * charRNN_size\n",
    "# question_char_state = tf.reshape(question_char_state, [-1, sequence_length, char_embed_dim])\n",
    "\n",
    "# answer_char_state = tf.concat(axis=1, values=[char_rnn_states2[0].h, char_rnn_states2[1].h] )\n",
    "# answer_char_state = tf.reshape(answer_char_state, [-1, sequence_length, char_embed_dim])\n",
    "\n",
    "charRNN_size = 40\n",
    "\n",
    "# _, qh1, _, qh2, _ = TimeDistributed(Bidirectional(LSTM(charRNN_size, return_state = True)))(question_char_embedded)\n",
    "# _, ah1, _, ah2, _ = TimeDistributed(Bidirectional(LSTM(charRNN_size, return_state = True)))(answer_char_embedded)\n",
    "\n",
    "daga1 = TimeDistributed(Bidirectional(LSTM(charRNN_size, return_state = True, return_sequences = False), merge_mode = None))(question_char_embedded)\n",
    "daga2 = TimeDistributed(Bidirectional(LSTM(charRNN_size, return_state = True)))(answer_char_embedded)\n",
    "\n",
    "# question_char_state = Concatenate()([qh1, qh2])\n",
    "# answer_char_state = Concatenate()([ah1, ah2])\n",
    "\n",
    "char_embed_dim = 2 * charRNN_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 20, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 20, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 10, 69)   4761        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 20, 10, 69)   4761        input_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,522\n",
      "Trainable params: 9,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs = [q_charVec, a_charVec], outputs = [question_char_embedded, answer_char_embedded])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
