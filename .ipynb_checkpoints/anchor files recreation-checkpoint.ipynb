{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_beg(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "def rem_end(text, suffix):\n",
    "    if text.endswith(suffix):\n",
    "        return text[:-len(suffix)]\n",
    "    return text\n",
    "\n",
    "def rem_both(text, pattern):\n",
    "    return rem_suffix(rem_prefix(text,pattern),pattern)\n",
    "\n",
    "def removeNonAscii(text):\n",
    "    out = ''\n",
    "    for c in text:\n",
    "        if ord(c) in range(128):\n",
    "            out += c\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section : 2 Progress: 444 / 747 / 0 \r"
     ]
    }
   ],
   "source": [
    "not_man_page = []\n",
    "\n",
    "for section in range(4,10):\n",
    "    directory = \"D:\\\\KGP\\\\man\" + str(section) + \"\\\\\"\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    all_files = os.listdir(directory)\n",
    "    all_files = set(all_files)\n",
    "    #all_extraction_files = {rem_end(each,'.txt') + '.html' for each in os.listdir('D:\\\\KGP\\\\Extractions\\\\man' + str(section) + '\\\\')}\n",
    "    #all_anchor_files = {rem_end(each,'.txt') + '.html' for each in os.listdir('D:\\\\KGP\\\\Extractions\\\\Anchors\\\\man' + str(section) + '\\\\')}\n",
    "    #completed_files = all_extraction_files & all_anchor_files\n",
    "    #all_files = all_files - completed_files\n",
    "    \n",
    "    for file in all_files:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"Section : %d Progress: %d / %d / %d \\r\" % (section, n, len(all_files), len(not_man_page)))\n",
    "            \n",
    "            #----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            sauce = open(directory + file, 'rb') \n",
    "            soup = bs.BeautifulSoup(sauce, 'lxml')\n",
    "            name_code = re.search(r'<b> *((NAME)|(Name)) *</b>([\\s\\S]*?)<pre>([\\s\\S]*?)</pre>', str(soup))\n",
    "            desc_code =  re.search('((<b> *DESCRIPTION *</b>)|(<b> *Description *</b>)|(<b> *Detailed *</b> <b> *Description *</b>))([\\s\\S]*?)<pre>([\\s\\S]*?)</pre>', str(soup))\n",
    "\n",
    "            if desc_code == None and name_code == None:\n",
    "                not_man_page.append(directory + file)\n",
    "                continue\n",
    "\n",
    "            # Find the code for the NAME section\n",
    "            if name_code == None:\n",
    "                name_code = soup.title.text\n",
    "            else:\n",
    "                name_code = name_code.group(5)\n",
    "\n",
    "            # Extract the title from the name code\n",
    "\n",
    "            title = name_code.replace('\\n','').strip()\n",
    "            if (title == 'Ubuntu Manpage:'):\n",
    "                ubu_man.append(directory + file)\n",
    "                continue\n",
    "\n",
    "            title = regex.search(r'(.*?)[ ]+([\\p{Pd}]|(<b>[\\p{Pd}]</b>))', title)\n",
    "            if title == None:\n",
    "                title = re.search(r'[^.]+', file).group(0).replace('_COLON_',':')\n",
    "            else:\n",
    "                title = title.group(1)\n",
    "                \n",
    "            all_title = title.split(', ')\n",
    "            \n",
    "            # Append '(section id)' to the title\n",
    "            for idx, title in enumerate(all_title):\n",
    "                all_title[idx] = all_title[idx].replace('<b>','').replace('</b>','').strip()\n",
    "                all_title[idx] = all_title[idx] + '(' + str(section) + ')'\n",
    "            \n",
    "            all_title = set(all_title)\n",
    "            all_title = {removeNonAscii(v) for v in all_title}\n",
    "\n",
    "            #----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Find anchor entities\n",
    "\n",
    "            # Find the 'SEE ALSO' section's html code \n",
    "            see_also_code = re.search(r'<b>SEE</b> <b>ALSO</b>([\\s\\S]*?)<pre>([\\s\\S]*?)</pre>', str(soup))\n",
    "            anchor_entities = set()\n",
    "\n",
    "            if (see_also_code != None):\n",
    "\n",
    "                see_also_code = see_also_code.group(2)\n",
    "\n",
    "                # Find the entities corresponding to href tags\n",
    "                anchor_entities = [each[1] + each[2] for each in re.findall(r'<a(.*?)>(.*?)</a></u>(\\([0-9]\\))', see_also_code)]\n",
    "\n",
    "                # Remove duplicates\n",
    "                anchor_entities = set(anchor_entities)\n",
    "\n",
    "                # Remove entity if entity(id) is already present\n",
    "                for each in anchor_entities.copy():\n",
    "                    if len(each) >= 3 and each[-3] == '(' and each[-1] == ')':\n",
    "                        anchor_entities.discard(each[:-3])\n",
    "            \n",
    "            anchor_entities = {removeNonAscii(v) for v in anchor_entities}\n",
    "\n",
    "             #----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Write the extractions of this manpage to a file\n",
    "\n",
    "            file_name = rem_end(file,'.html') + '.txt'\n",
    "\n",
    "            with open('D:\\\\KGP\\\\Extractions\\\\Anchors\\\\man' + str(section) + '\\\\' + file_name, 'w') as file_int:\n",
    "                fieldnames = ['title','links']\n",
    "                #fieldnames = [v.encode('utf8') for v in fieldnames]\n",
    "                csv_writer = csv.DictWriter(file_int, fieldnames=fieldnames, delimiter = '\\t')\n",
    "                csv_writer.writeheader()\n",
    "\n",
    "                for title in all_title:\n",
    "                    new_dict = {}\n",
    "                    new_dict['title'] = title\n",
    "                    new_dict['links'] = anchor_entities\n",
    "                    #csv_writer.writerow({k:v.encode('utf8') for k,v in new_dict.items()})\n",
    "                    csv_writer.writerow(new_dict)\n",
    "                    \n",
    "            #----------------------------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            n += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(directory + file)\n",
    "            traceback.print_exc()\n",
    "            sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
